# Graph Neural Networks and Analytics

## Introduction
Learn to run Graph Neural Network (GNN) training on CPUs in single and distributed mode. The workflow reads tabular data, ingests it into graph format, and then uses a GNN to learn embeddings used as rich features in a downstream task.

This workflow is used by the [Fraud Detection Reference Kit](https://github.com/intel/credit-card-fraud-detection).

Check out more workflow examples and reference implementations in the [Dev
Catalog](https://developer.intel.com/aireferenceimplementations).

## Solution Technical Overview
Graph Neural Networks are effective models for generating node/edge embeddings that can be used as rich features to improve accuracy of downstream tasks.
This workflow provides a step-by-step example for how GNNs can be used in fraud detection to extract node embeddings for all entities (credit cards and merchant) based on the graph structure defined by the transactions between them.

General steps:
- **Graph Construction/Graph Partition**: Conversion of tabular data into a set of files (nodes.csv, edges.csv and meta.yml). These CSV files form a CSVDataset for ingestion into [Deep Graph Library (DGL)](https://www.dgl.ai/) graph.
- **GNN Training**: Training of GNN GraphSAGE model for self-supervised transductive link prediction task. If this is performed on a cluster of machines this training is preceded by graph partitioning.
- **Emb Mapping**: Mapping of generated node embeddings into original tabular dataset.

## Solution Technical Details
Use cases such as fraud detection, are characterized by class imbalance in their datasets that makes training predictor models directly with those labels difficult. This GNN workflow shows an example of how a self-supervised task can be formulated (instead of using the hard imbalanced labels) to learn entity features that capture the graph structure for use by a downstream predictor model such as XGBoost. In this workflow, the self-supervised task consists of link prediction where the edges in the graph are used as positive examples and non-existent edges as negative examples.

GNN workflow ingests tabular data where each row corresponds to a transaction between two types of entities: cards and merchants, and generates a graph where entities are the nodes (that are featureless), and transactions constitute the edges with the associated transaction feature attributes.

The GNN model consists of a learnable embedding layer followed by an encoder, implemented as a 2-layer GraphSAGE model, and a decoder, implemented as a 3-layer multilayer perceptron (MLP) with a single output for link prediction. During training, positive and negative neighbor sampling is used to generate the training examples. We use a Receiver Operating Characteristic Area Under Curve score (ROC AUC) as the metric to evaluate the quality of the embeddings in predicting if two entities should be connected. The ultimate measure of how useful these embeddings are in predicting fraud needs to be measured by the downstream predictor model, since this workflow is not using the fraud labels directly.

After several epochs of training, we run GNN inference on the entire graph without neighbor sampling and use the last layer activations generated by the model as node embeddings for nodes of the graph. These embeddings can be mapped to the entities in the tabular data input and used as node features for a downstream prediction task.

![GNN_WF](docs/GNN_WF.png)

## Validated Hardware Details
There are workflow-specific hardware and software setup requirements depending on how the workflow is run. Bare metal development system and Docker image running
locally have the same system requirements.

| Recommended Hardware         | Precision  |
| ---------------------------- | ---------- |
| Intel® 1st, 2nd, 3rd, and 4th Gen Xeon® Scalable Performance processors| FP32 |

For distributed training a high-speed fabric across nodes (e.g., OPA, Mellanox) is recommended.

Workflow has been tested on OS Rocky Linux v8.7 and Ubuntu 20.04

## How it Works
This GNN workflow reads the tabular data, ingests it into the graph, and then use a GNN to learn the embeddings to enrich the features for the following task, where the whole process can be configured by the user using yaml configuration files and it supports running in different ways:
- Run single node bare metal
- Run single node using Docker
- Run bare metal on a cluster of machines
- Run docker on a cluster of machines

The selection between these different modes can be done in the `workflow-config.yaml`.

In these sections you will find instructions on how to update the configuration yaml files to run this workflow.
### Update workflow-config.yaml
`workflow-config.yaml` is the main configuration file for the user to specify:
1. Runtime environment (i,e number of nodes in cluster, IPs, bare metal/docker, ...)
2. Directories for inputs, outputs and configuration files
3. Configure what stages of the workflow to execute. A user may run all stages the first time but may want to skip building or partitioning a graph in later training experiments to save time.

Please refer to the `workflow-config.yaml` for a detailed description of input configurations.

### Update model-training.yaml
In `model-training.yaml` user can specify:
1. Dataloader, sampler and model parameters (i,e batch size, sampling fanout, learning rate)
2. Training hyperparamets (i,e number of epochs)
3. DGL specific parameters for distributed training

Please refer to the `model-training.yaml` for a detailed description of input configurations.

## Get Started
Start by defining an environment variable that will store the workspace path, this can be an existing directory or one to be created in further steps. This `ENVVAR` will be used for all the commands executed using absolute paths.

E.g.
```bash
export WORKSPACE=~/work
```

For distributed pipelines that require more than one kind of storage (NFS and localdisk for instance) an environment variable:

E.g.
```bash
export WORKSPACE=~/work
export NFS_DIR=$WORKSPACE/nfs-work
export LOCAL_DIR=$WORKSPACE/local-work
```

### Download the Workflow Repository
Create a working directory for the workflow and clone the [Main
Repository](https://github.com/intel/graph-neural-networks-and-analytics) into your working
directory.

```bash
mkdir -p $WORKSPACE && cd $WORKSPACE
git clone https://github.com/intel/graph-neural-networks-and-analytics
cd graph-neural-networks-and-analytics
git checkout main
```

### Download the Datasets

The input to this workflow is tabular data in CSV format where each row corresponds to a transaction between two entities. In the case of the [IBM/tabformer](https://github.com/IBM/TabFormer/blob/main/data/credit_card/transactions.tgz) dataset used by [Fraud Detection Reference Kit](https://github.com/intel/credit-card-fraud-detection) it consists of credit card transaction. Each transaction includes the IDs of the entities involved (Card and Merchant) and the edge features (amount of the transaction, date, etc.).

```bash
#create the dataset folder and download dataset
mkdir -p $$WORKSPACE/<recommended dataset folder name>
cd $WORKSPACE/<recommended dataset folder name>
<downloading datasets please refer to the IBM/tabformer and Fraud Detection Reference Kit>
cd $WORKSPACE
```

There are two options for processing the input data:

#### 2a. Run as part of Fraud Detection Reference Kit (with Tabformer dataset)

If you are running this workflow as part of the [Fraud Detection Reference Kit](https://github.com/intel/credit-card-fraud-detection) the input to this workflow will be the output of the data preprocesssing (featurized edge data) stage.

OR

#### 2b. Run standalone (with Tabformer dataset)
If you want to run this workflow in standalone mode, download the synthetic credit card transaction dataset [IBM/tabformer](https://github.com/IBM/TabFormer/blob/main/data/credit_card/transactions.tgz) and use the utility script below to featurize the data using pandas.

```bash
#run this script to create conda environment
./script/build_dgl1_env.sh
conda activate dgl1.0
#path to downloaded data. Include filename
DATA_IN=$WORKSPACE/<recommended dataset folder name>/card_transaction.v1.csv
#path for processed_data.csv. Include filename
PROCESSED_DATA=$WORKSPACE/<recommended dataset folder name>/processed_data.csv

#read raw data, perform edge featurization and generate CSVDataset files for ingesting graph
./script/run_data_prep.sh $DATA_IN $PROCESSED_DATA
```

---

## Suppotered Runtime Environment
You can execute the references pipelines using the following environments:
* Docker
* Bare metal

### Run Using Docker
Follow these instructions to set up and run our provided Docker image.
For running on bare metal, see the [bare metal instructions](#run-using-bare-metal)
instructions.

#### Set Up Docker Engine
You'll need to install Docker Engine on your development system.
Note that while **Docker Engine** is free to use, **Docker Desktop** may require
you to purchase a license.  See the [Docker Engine Server installation
instructions](https://docs.docker.com/engine/install/#server) for details.

Before you can run docker in a distributed cluster, you need to configure passwordless ssh access across machines and have a distributed file system so the conda environment, data and files can be accessed across multiple machines.
- [How to set up passwordless ssh](https://linuxize.com/post/how-to-setup-passwordless-ssh-login/)
- [How to set up distributed file system](https://github.com/dmlc/dgl/tree/1.0.0/examples/pytorch/graphsage/dist)

#### Set Up Docker Image
Pull the provided docker image.
```bash
docker pull intel/ai-workflows:pa-fraud-detection-gnn
```

If your environment requires a proxy to access the internet, export your
development system's proxy settings to the docker environment:
```bash
export DOCKER_RUN_ENVS="-e ftp_proxy=${ftp_proxy} \
  -e FTP_PROXY=${FTP_PROXY} -e http_proxy=${http_proxy} \
  -e HTTP_PROXY=${HTTP_PROXY} -e https_proxy=${https_proxy} \
  -e HTTPS_PROXY=${HTTPS_PROXY} -e no_proxy=${no_proxy} \
  -e NO_PROXY=${NO_PROXY} -e socks_proxy=${socks_proxy} \
  -e SOCKS_PROXY=${SOCKS_PROXY}"
```

#### Run Docker Image
Run the workflow using the `docker run` command as shown:
- single node
```bash
docker run --shm-size=200g --network host --name gnn\
          -v "${repoPath}":/host \
          -v "${env_data_path}":/DATA_IN \
          -v "${env_out_path}":/DATA_OUT \
          -v "${env_tmp_path}":/GNN_TMP \
          -v "${env_config_path}":/CONFIGS \
          -it ${env_docker_image} ./host/script/run_gnn_wf_docker.sh /CONFIGS/workflow-config.yaml
```
- mulitple nodes
```bash
# start docker container in each node
${repoPath}/script/start_work_node.sh ${repoPath}/configs/workflow-config.yaml
# execute the workflow in the master node
docker exec -it gnn /host/script/run_gnn_dist_wf_docker.sh /host/configs/workflow-config.yaml
```

#### Run Workflow
The simplest way to trigger the workflow of the docker mode through one command script with the configuration introduced in the ["How it Works"](#get-started):
```bash
./run-workflow.sh ./configs/workflow-config.yaml
```

### Run Using Bare Metal
Follow these instructions to set up and run this workflow on your own development
system. For running a provided Docker image with Docker, see the [Docker instructions](#run-using-docker).

#### Set Up System Software
Our examples use the `conda` package and environment on your local computer.
If you don't already have `conda` installed, see the [Conda Linux installation
instructions](https://docs.conda.io/projects/conda/en/stable/user-guide/install/linux.html).

In addition, before you can run in a distributed cluster, you need to configure passwordless ssh access across machines and have a distributed file system so the conda environment, data and files can be accessed across multiple machines. 
- [How to set up passwordless ssh](https://linuxize.com/post/how-to-setup-passwordless-ssh-login/)
- [How to set up distributed file system](https://github.com/dmlc/dgl/tree/1.0.0/examples/pytorch/graphsage/dist)

Have numactl installed on your system. You can use `sudo apt-get install numactl` (Ubuntu) `dnf install numactl` (CentOS)

#### Set Up Workflow
Run these commands to set up the workflow's conda environment and install required software:
```bash
#run this script to create conda environment
cd $WORKSPACE/graph-neural-networks-and-analytics
./script/build_dgl1_env.sh
```

#### Run Workflow
Once the prerequisits have been met and the [3 configuration files](#how-it-works) have been updated to execute the workflow, use these commands to run the workflow:
```
./run-workflow.sh ./configs/workflow-config.yaml
```

### Expected Output
#### Build Graph
The successful execution of this stage will create the below contents under ${env_tmp_path} directory specified in `workflow-config.yaml`:
```
./sym_tabformer_hetero_CSVDataset/
├── edges_0.csv
├── edges_1.csv
├── meta.yaml
├── nodes_0.csv
└── nodes_1.csv
```
#### Partition Graph (Distributed only)
The successful execution of this stage will create the below contents under ${env_tmp_path}/partitions directory:
```
tabformer_2parts/
├── emap.pkl
├── nmap.pkl
├── part0
│   ├── edge_feat.dgl
│   ├── graph.dgl
│   └── node_feat.dgl
├── part1
│   ├── edge_feat.dgl
│   ├── graph.dgl
│   └── node_feat.dgl
└── tabformer_full_homo.json
```
#### GNN Training
The successful training will show the epoch times as it progresses along with "roc_auc" scores. The training logs are saved under:
```bash
#the single node logs can be found
${WORKSPACE}/graph-neural-networks-and-analytics/logs
#the distributed training log can be found at:
${WORKSPACE}/graph-neural-networks-and-analytics/logs_dist
```
#### Mapping embeddings to input tabular data
The "map_save" stage generates a CSV file combining the input tabular data and the node embeddings generated by this GNN workflow. In the case of Tabformer datast this file will have 154 features per transaction and can be used for the fraud prediction downstream task. For an example of fraud detection with XGBoost using these gnn_features, refer to [Fraud Detection Reference Kit](https://github.com/intel/credit-card-fraud-detection).

The mappings happening in this stage are:
1) the mapping from the local to global graph IDs (node/edge ID's) and
2) the mapping between global graph ID's and the card and merchant ID's in the tabular dataset.

The successful mapping of the embeddings will show the following details
```
Loading embeddings from file and adding to preprocessed CSV file
CSV output shape:  (24198836, 154)
Time to append node embeddings to edge features CSV <this will vary based on system>
```
The output file (~35GB) can be found on your host system's output directory indicated by ${env_out_path} in `workflow-config.yaml`:
```bash
#resulting file with edge features and node embeddings
$OUT_DIR/tabular_with_gnn_emb.csv
```
                             
## Summary and Next Step
We have seen how tabular data such as credit card transactions can be modeled as a graph and how GNNs can be used to generate node embeddings used to boost accuracy of downstream tasks.
In addition we have shown how to bring your own tabular data and perform this GNN training on a container or on bare metal as a single node or multiple nodes by modifying a set of configuration files.

### How to customize this Workflow

#### Adopt to your dataset
You can bring your own tabular transaction dataset to be used with this workflow. For the workflow to be able to model the graph correctly user needs to update the
`tabular2graph.yaml` to include the column names for the node IDs, the edge types (triplets of (src_types, edge_type, dst_type), the column name for the label, list of column names for edge features, etc.

Please refer to the `workflow-config.yaml` for a detailed description of input configurations.

These are the minimum requiremenets on tabular dataset:
- CSV file including header with column names. User will use the `tabular2graph.yaml` to indicate what are the column names corresponding to entity IDs, label, features...
- Each row should represent a link in the graph thus including at minimum the IDs for the two connecting entities and a column indicating the train/val/test split using 0,1,2 values respectively.
- The number of node types and edge types is not constrained and will be provided by user in the "tabular2graph.yaml"
- label, column (edge) features are optional and will be included in the CSVDataset if provided in the `tabular2graph.yaml`
- Because current workflow is useful to model unsupervised link prediction tasks there is currently no support for other type of graph inputs such as multi graph.

## Learn More
For more information about Graph Neural Networks and Analytics workflow or to read about other relevant workflow examples, see these guides and software resources:

- [Fraud Detection Reference Kit](https://github.com/intel/credit-card-fraud-detection)
- [Classic ML Workflow](https://github.com/intel/recommender-system-with-distributed-classical-ml)

## Troubleshooting
1. Distributed training uses Pytorch with GLOO backend. If it gives a Connection Refused RuntimeError from third_party/gloo you may try adding `--extra_envs GLOO_SOCKET_IFNAME=eth0` in run_dist_train.sh after launch.py. Replace `eth0` above with the right network interface in your system.
2. If distributed training gets interrupted and cannot restart training successfully afterwards you may need to manually kill python processes in the remote machines.
3. If you see a TargetEncoder python import error when running run_data_prep.sh you can install it into your dgl1.0 conda environment using `conda install -c conda-forge category_encoders`.
4. If you get a Permission denied error when running a bash script such as `./script/run_build_graph.sh`, it can be fixed by `chmod +x ./script/run_build_graph.sh`

## Support
If you have questions or issues about this workflow, contact the [Support Team](support_forum_link). If there is no support forum, and we want developers to use GitHub issues to submit bugs and enhancement requests, put a link to that GitHub repo's issues, something like this:

The Graph Neural Networks and Analytics team tracks both bugs and
enhancement requests using [GitHub
issues](https://github.com/intel/graph-neural-networks-and-analytics/issues). Before submitting a suggestion or bug report, search the [GNN GitHub issues](https://github.com/intel/graph-neural-networks-and-analytics/issues) to see if your issue has already been reported.

---

\*Other names and brands may be claimed as the property of others.
[Trademarks](https://www.intel.com/content/www/us/en/legal/trademarks.html).
