env:
  num_node: 1
  node_ips: #pls make sure that the ip doesn't contain space in the end
    - 123.1.2.3
    - 123.1.2.4
  tmp_path: /home/${USER}/beta_test
  #tmp_path used to save model, embeddings, partitions...
  data_path: /home/${USER}/beta_test
  in_data_filename: processed_data.csv
  #data_path should contain the in_data_filename (processed_data.csv)
  out_path: /home/${USER}/beta_test
  #out_path will contain the output csv with the tabular data and new node embeddings
  config_path: /home/${USER}/applications.ai.appliedml.workflow.GNNandAnalytics/configs
  #for single node docker exec paths need to be on /localdisk (or NFS with full permissions)
  #for distributed exec paths need to be on NFS along with code repo
  bare_metal: True
  #bare_metal=False means run using docker container
  #docker_image: intel/ai-workflows:eap-fraud-detection-gnn
  docker_image: gnn-wf
  train_config_file: model-training.yaml
  tabular2graph_config_file: tabular2graph.yaml

#first time run all stages but later you can set stages to False to run with prior results
#i.e skip building graph and partitions to save time and jump directly to training
single:
  build_graph: True
  #build_graph stage generates CSVDataset files for DGL to ingest data as graph
  gnn_training: True
  map_save: True
  #map_save stage performs the mapping of the computed node embeddings to the input tabular data file

distributed:
  build_graph: True
  partition_graph: True
  #partition_graph stages uses random partition algorithm to generate "num_parts" subgraphs for distributed training
  gnn_training: True
  map_save: True
  num_parts: 2
  #during training num_parts should match the env_num_nodes. If running partitioning stage by itself you can modify the number
  #(i,e if you want to do multiple graph pre-partitioning before starting the training runs)

graph:
  #provide a name for the graph
  CSVDataset_name: sym_tabformer_hetero_CSVDatasets
  name: tabformer_full_homo
